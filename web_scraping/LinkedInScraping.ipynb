{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c319f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "# from urllib.request import urlopen\n",
    "# import requests \n",
    "import re\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d662b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Logging into LinkedIn\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "time.sleep(5)\n",
    "\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "username.send_keys(\"anrs0310@proton.me\") # Enter Your Email Address\n",
    "\n",
    "pword = driver.find_element(By.ID, \"password\")\n",
    "pword.send_keys(\"SrqoBtP2\")\t # Enter Your Password\n",
    "\n",
    "driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e61a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "exceldata = pd.read_csv(\"udemy-datacleaning.csv\")\n",
    "LinkedIn_company_url = []\n",
    "Index_company_datas = []\n",
    "LinkedIn_personal_url = []\n",
    "Index_personal_datas = []\n",
    "\n",
    "for idx in exceldata.index:\n",
    "    linkedin = exceldata['linkedin_url'][idx]\n",
    "    Index_data = exceldata['index_number'][idx]\n",
    "\n",
    "    to_check = \"company\"\n",
    "\n",
    "    if re.search(r'\\b' + re.escape(to_check) + r'\\b', linkedin):\n",
    "        LinkedIn_company_url.append(linkedin)\n",
    "        Index_company_datas.append(Index_data)\n",
    "    else:\n",
    "        LinkedIn_personal_url.append(linkedin)\n",
    "        Index_personal_datas.append(Index_data)\n",
    "\n",
    "print(f\"Number of company urls = {len(LinkedIn_company_url)}\")\n",
    "print(f\"Number of personal urls = {len(LinkedIn_personal_url)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e307be",
   "metadata": {},
   "source": [
    "# Code for scraping personal account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a50003",
   "metadata": {},
   "outputs": [],
   "source": [
    "Index_personal = []\n",
    "Instructor_LinkedIn_url = []\n",
    "Instructor_name = []\n",
    "Instructor_worksat = []\n",
    "Instructor_followers = []\n",
    "Instructor_connections = []\n",
    "About_Instructor = []\n",
    "Post1_frequency = []\n",
    "Post1_numberof_reactions = []\n",
    "Post1_numberof_comments = []\n",
    "Post2_frequency = []\n",
    "Post2_numberof_reactions = []\n",
    "Post2_numberof_comments = []\n",
    "Post3_frequency = []\n",
    "Post3_numberof_reactions = []\n",
    "Post3_numberof_comments = []\n",
    "\n",
    "\n",
    "durations = [[],[],[]]\n",
    "reactions = [[],[],[]]\n",
    "comments = [[],[],[]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ebfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# No Post : https://linkedin.com/in/pianorosa\n",
    "#profile_url = [\"https://www.linkedin.com/in/philip-ebiner-06728413/\",\"https://linkedin.com/in/vishukamble\",\"https://linkedin.com/in/svekis\",\"https://www.linkedin.com/in/gilseone/\"]\n",
    "\n",
    "count1 = 0\n",
    "\n",
    "\n",
    "# id 427 = 'https://linkedin.com/profile/preview?locale=en_US&trk=prof-0-sb-preview-primary-button' it will be empty set \n",
    "\n",
    "exclusion_set = {'https://linkedin.com/profile/preview?locale=en_US&trk=prof-0-sb-preview-primary-button'} \n",
    "\n",
    "for id,p_url in zip(Index_personal_datas,LinkedIn_personal_url):\n",
    "\n",
    "    count1 = count1 + 1\n",
    "    print(f'{count1=} {id=} {p_url=}')\n",
    "\n",
    "    if p_url in exclusion_set:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "\n",
    "        Index_personal.append(id)\n",
    "\n",
    "        Instructor_LinkedIn_url.append(p_url)\n",
    "\n",
    "        driver.get(p_url)\t # this will open the link\n",
    "\n",
    "        # https://www.selenium.dev/documentation/webdriver/waits/\n",
    "        # driver.implicitly_wait(5)\n",
    "\n",
    "        time.sleep(6)\n",
    "\n",
    "        # Extracting the HTML of the complete introduction box\n",
    "        # that contains the name, company name, and the location\n",
    "        src = driver.page_source\n",
    "        \n",
    "        # Now using beautiful soup\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        # time.sleep(15)\n",
    "\n",
    "        error_404 = soup.find('body', {'id': 'error404'})\n",
    "        page_not_found = soup.find('div', {'class': 'page-not-found'})\n",
    "        intro = soup.find('div', {'class': 'pv-text-details__left-panel'})\n",
    "\n",
    "        if error_404 or page_not_found or not intro:\n",
    "            print(\"404\")\n",
    "            Instructor_name.append(\"N/A\")\n",
    "            Instructor_worksat.append(\"N/A\")\n",
    "            Instructor_connections.append(\"N/A\")\n",
    "            Instructor_followers.append(\"N/A\")\n",
    "            About_Instructor.append(\"N/A\")\n",
    "            for i in range(3):\n",
    "                durations[i].append(\"N/A\")\n",
    "                reactions[i].append(\"N/A\")\n",
    "                comments[i].append(\"N/A\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "        name_loc = intro.find(\"h1\")\n",
    "        if name_loc:\n",
    "            name = name_loc.get_text().strip()\n",
    "            Instructor_name.append(name)\n",
    "        else:\n",
    "            Instructor_name.append(\"\")\n",
    "\n",
    "\n",
    "        works_at_loc = intro.find(\"div\", {'class': 'text-body-medium'})\n",
    "        if works_at_loc:\n",
    "            works_at = works_at_loc.get_text().strip()\n",
    "            Instructor_worksat.append(works_at)\n",
    "        else:\n",
    "            Instructor_worksat.append(\"\")\n",
    "\n",
    "        \n",
    "        about_section = soup.find('div',{'class': 'pv-shared-text-with-see-more full-width t-14 t-normal t-black display-flex align-items-center'})\n",
    "        if about_section:\n",
    "            absec = about_section.get_text().strip()\n",
    "            About_Instructor.append(absec)\n",
    "        else:\n",
    "            About_Instructor.append(\"\")\n",
    "\n",
    "\n",
    "        \n",
    "        connections = soup.find('ul', {'class': 'pv-top-card--list pv-top-card--list-bullet'}).find('span',{'class' :'t-black--light'})\n",
    "        if connections:\n",
    "            connect = connections.get_text().strip()\n",
    "            Instructor_connections.append(connect)\n",
    "        else:   \n",
    "            Instructor_connections.append(\"\")\n",
    "\n",
    "\n",
    "        \n",
    "        followers_count = soup.find('p', {'class': 'pvs-header__subtitle pvs-header__optional-link text-body-small'}).find('span',{'class':'visually-hidden'})\n",
    "        if followers_count:\n",
    "            fc = followers_count.get_text().strip()\n",
    "            Instructor_followers.append(fc)\n",
    "        else:\n",
    "            Instructor_followers.append(\"\")\n",
    "\n",
    "\n",
    "        posts = soup.find('ul', {'class': 'display-flex flex-wrap list-style-none justify-space-between'})\n",
    "        if not posts:\n",
    "            for i in range(3):\n",
    "                durations[i].append(\"N/A\")\n",
    "                reactions[i].append(\"N/A\")\n",
    "                comments[i].append(\"N/A\")\n",
    "            continue\n",
    "            \n",
    "        post1 = posts.findAll('li', {'class': 'profile-creator-shared-feed-update__mini-container'})\n",
    "\n",
    "        for i in range(3): # post1, post2, post3\n",
    "\n",
    "            if i > len(post1)-1:\n",
    "                durations[i].append(\"\")\n",
    "                reactions[i].append(\"\")\n",
    "                comments[i].append(\"\")\n",
    "                continue\n",
    "            element = post1[i]\n",
    "\n",
    "            post_duration_data = element.find('span', {'class': 'feed-mini-update-contextual-description__text'})\n",
    "            if post_duration_data:\n",
    "                # Find the nested span with class 'visually-hidden' and extract its text\n",
    "                post_duration_text = post_duration_data.find(\"span\", {\"class\": \"visually-hidden\"}).text.strip()\n",
    "                durations[i].append(post_duration_text)\n",
    "            else:\n",
    "                durations[i].append(\"\")\n",
    "\n",
    "            reaction_data = element.find('span', {'class' : 'social-details-social-counts__reactions-count' })\n",
    "            if reaction_data:\n",
    "                # Find the nested span with class 'visually-hidden' and extract its text\n",
    "                reaction_text = reaction_data.get_text().strip()\n",
    "                reactions[i].append(reaction_text)\n",
    "            else:    \n",
    "                reactions[i].append(\"\")\n",
    "\n",
    "\n",
    "            comments_data = element.find('li', {'class' : 'social-details-social-counts__item social-details-social-counts__comments social-details-social-counts__item--with-social-proof'})\n",
    "            if comments_data:\n",
    "                # Find the nested span with class 'visually-hidden' and extract its text\n",
    "                comment_text = comments_data.get_text(strip=True)  # Get the text and remove leading/trailing whitespace\n",
    "                comments[i].append(comment_text)\n",
    "            else:\n",
    "                comments[i].append(\"\")\n",
    "        \n",
    "        time.sleep(20)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Timeout occurred\")\n",
    "\n",
    "        print(e)\n",
    "\n",
    "\n",
    "Post1_frequency= durations[0]\n",
    "Post1_numberof_reactions = reactions[0]\n",
    "Post1_numberof_comments = comments[0]\n",
    "\n",
    "Post2_frequency= durations[1]\n",
    "Post2_numberof_reactions = reactions[1]\n",
    "Post2_numberof_comments = comments[1]\n",
    "\n",
    "Post3_frequency= durations[2]\n",
    "Post3_numberof_reactions = reactions[2]\n",
    "Post3_numberof_comments = comments[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'Index' : Index_personal,\n",
    "        'LinkedIn Url' : Instructor_LinkedIn_url,\n",
    "    'Instructor Name' : Instructor_name,\n",
    "'Instructor work status' : Instructor_worksat,\n",
    "'Number of followers' : Instructor_followers,\n",
    "'Number of connections' : Instructor_connections,\n",
    "'About section' : About_Instructor,\n",
    "'Frequency of the latest post' : Post1_frequency,\n",
    "'Number of reactions got for latest post' : Post1_numberof_reactions,\n",
    "'Number of comments got for latest post' : Post1_numberof_comments,\n",
    "'Frequency of the second latest post' : Post2_frequency,\n",
    "'Number of reactions got for second latest post' : Post2_numberof_reactions,\n",
    "'Number of comments got for second latest post' : Post2_numberof_comments,\n",
    "'Frequency of the third latest post' : Post3_frequency,\n",
    "'Number of reactions got for third latest post' : Post3_numberof_reactions,\n",
    "'Number of comments got for third latest post' : Post3_numberof_comments }\n",
    "\n",
    "li_per_df = pd.DataFrame(data1)\n",
    "li_per_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b102e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_per_df.to_csv('LinkedIn_instructor_data7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976484c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('LinkedIn_instructor_data123456.csv')\n",
    "df4 = pd.read_csv('LinkedIn_instructor_data7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_df2 = pd.concat([df3, df4], ignore_index=True)\n",
    "\n",
    "appended_df2.to_csv('LinkedIn_instructor_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_data = pd.read_csv('LinkedIn_instructor_data.csv')\n",
    "personal_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8d4f9",
   "metadata": {},
   "source": [
    "# Code for scraping company pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "Index_company = []\n",
    "Company_LinkedIn_url = []\n",
    "Company_name = []\n",
    "Company_worksat = []\n",
    "Company_followers = []\n",
    "Number_of_employees = []\n",
    "About_Company = []\n",
    "CPost1_frequency = []\n",
    "CPost1_numberof_reactions = []\n",
    "CPost1_numberof_comments = []\n",
    "CPost2_frequency = []\n",
    "CPost2_numberof_reactions = []\n",
    "CPost2_numberof_comments = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b56e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "count2 = 0\n",
    "\n",
    "for id,p_url in zip(Index_company_datas,LinkedIn_company_url):\n",
    "\n",
    "    count2 = count2 + 1\n",
    "    print(f'{count2=} {id=} {p_url=}')\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        Index_company.append(id)\n",
    "\n",
    "        Company_LinkedIn_url.append(p_url)\n",
    "\n",
    "        driver.get(p_url)\t # this will open the link\n",
    "\n",
    "        # https://www.selenium.dev/documentation/webdriver/waits/\n",
    "        # driver.implicitly_wait(5)\n",
    "\n",
    "        time.sleep(6)\n",
    "\n",
    "        # Extracting the HTML of the complete introduction box\n",
    "        # that contains the name, company name, and the location\n",
    "        src = driver.page_source\n",
    "        \n",
    "        # Now using beautiful soup\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        # time.sleep(15)\n",
    "\n",
    "        error_404 = soup.find('body', {'id': 'error404'})\n",
    "        page_not_found = soup.find('div', {'class': 'page-not-found'})\n",
    "        intro = soup.find('div', {'class': 'ph5 pb5'})\n",
    "\n",
    "        if error_404 or page_not_found or not intro:\n",
    "            print(\"404\")\n",
    "            Company_name.append(\"N/A\")\n",
    "            Company_worksat.append(\"N/A\")\n",
    "            Company_followers.append(\"N/A\")\n",
    "            Number_of_employees.append(\"N/A\")\n",
    "            About_Company.append(\"N/A\")\n",
    "            CPost1_frequency.append(\"N/A\")\n",
    "            CPost1_numberof_reactions.append(\"N/A\")\n",
    "            CPost1_numberof_comments.append(\"N/A\")\n",
    "            CPost2_frequency.append(\"N/A\")\n",
    "            CPost2_numberof_reactions.append(\"N/A\")\n",
    "            CPost2_numberof_comments.append(\"N/A\")\n",
    "            continue\n",
    "                    \n",
    "\n",
    "       \n",
    "        comp_name = intro.find(\"h1\", {'class': 'ember-view text-display-medium-bold org-top-card-summary__title full-width'})\n",
    "        if comp_name:\n",
    "            comp_name_text = comp_name.get_text().strip()\n",
    "            Company_name.append(comp_name_text)\n",
    "        else:\n",
    "            Company_name.append(\"\")\n",
    "        \n",
    "\n",
    "        work_status = intro.find(\"p\", {'class': 'org-top-card-summary__tagline'})\n",
    "        if work_status:\n",
    "            ws_text = work_status.get_text().strip()\n",
    "            Company_worksat.append(ws_text)\n",
    "        else:\n",
    "            Company_worksat.append(\"\")\n",
    "\n",
    "    \n",
    "        followers_text = intro.findAll(\"div\", {'class': 'org-top-card-summary-info-list__info-item'})\n",
    "        if followers_text:\n",
    "            fc_text = \"\"\n",
    "            for ft in followers_text:\n",
    "                if \"followers\" in ft.get_text().strip():\n",
    "                    fc_text = ft.get_text().strip()\n",
    "            Company_followers.append(fc_text)\n",
    "        else:\n",
    "             Company_followers.append(\"\")\n",
    "\n",
    "\n",
    "        employee = intro.find(\"span\", {'class': 't-normal t-black--light link-without-visited-state link-without-hover-state'})\n",
    "        if employee:\n",
    "            emp_text = employee.get_text().strip()\n",
    "            Number_of_employees.append(emp_text)\n",
    "        else:\n",
    "            Number_of_employees.append(\"\")\n",
    "\n",
    "        about_section = soup.find('div',{'class': 'ember-view organization-about-module__content-consistant-cards-description'})\n",
    "        if about_section:\n",
    "            abtsec_text = about_section.get_text().strip('\\n')\n",
    "            About_Company.append(abtsec_text)\n",
    "        else:\n",
    "            About_Company.append(\"\")\n",
    "\n",
    "\n",
    "        posts = soup.find('ul', {'class': 'artdeco-carousel__slider ember-view'})\n",
    "\n",
    "        if not posts:\n",
    "            CPost1_frequency.append(\"N/A\")\n",
    "            CPost1_numberof_reactions.append(\"N/A\")\n",
    "            CPost1_numberof_comments.append(\"N/A\")\n",
    "            CPost2_frequency.append(\"N/A\")\n",
    "            CPost2_numberof_reactions.append(\"N/A\")\n",
    "            CPost2_numberof_comments.append(\"N/A\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "\n",
    "            post1_data = posts.find('li', {'class': 'artdeco-carousel__item active ember-view'})\n",
    "        except:\n",
    "            CPost1_frequency.append(\"\")\n",
    "            CPost1_numberof_reactions.append(\"\")\n",
    "            CPost1_numberof_comments.append(\"\")\n",
    "\n",
    "        try:\n",
    "            post1_duration_data = post1_data.find('div', {'class': \"update-components-text-view break-words\"}).get_text().strip()\n",
    "            CPost1_frequency.append(post1_duration_data)\n",
    "        except:\n",
    "            CPost1_frequency.append(\"\")\n",
    "\n",
    "        try:\n",
    "            post1_reaction_data = post1_data.find('span', {'class': \"social-details-social-counts__reactions-count\"}).get_text().strip()\n",
    "            CPost1_numberof_reactions.append(post1_reaction_data)\n",
    "        except:\n",
    "            CPost1_numberof_reactions.append(\"\")\n",
    "        \n",
    "        try:\n",
    "            post1_comments_data = post1_data.find('li', {'class': \"social-details-social-counts__item social-details-social-counts__comments\"}).get_text().strip()\n",
    "            CPost1_numberof_comments.append(post1_comments_data)\n",
    "        except:\n",
    "            CPost1_numberof_comments.append(\"\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            post2_data = posts.find('li', {'class': 'artdeco-carousel__item ember-view'})\n",
    "        except:\n",
    "            CPost2_frequency.append(\"\")\n",
    "            CPost2_numberof_reactions.append(\"\")\n",
    "            CPost2_numberof_comments.append(\"\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            post2_duration_data = post2_data.find('div', {'class': \"update-components-text-view break-words\"}).get_text().strip()\n",
    "            CPost2_frequency.append(post2_duration_data)\n",
    "        except:\n",
    "            CPost2_frequency.append(\"\")\n",
    "\n",
    "        try:\n",
    "            post2_reaction_data = post2_data.find('span', {'class': \"social-details-social-counts__reactions-count\"}).get_text().strip()\n",
    "            CPost2_numberof_reactions.append(post2_reaction_data)\n",
    "        except:\n",
    "            CPost2_numberof_reactions.append(\"\")\n",
    "        \n",
    "        try:\n",
    "            post2_comments_data = post2_data.find('li', {'class': \"social-details-social-counts__item social-details-social-counts__comments\"}).get_text().strip()\n",
    "            CPost2_numberof_comments.append(post2_comments_data)\n",
    "        except:\n",
    "            CPost2_numberof_comments.append(\"\")\n",
    "               \n",
    "        time.sleep(20)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Timeout occurred\")\n",
    "\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b85cd",
   "metadata": {},
   "source": [
    "id=42 p_url='https://linkedin.com/company/tokenmeister/'\n",
    "id=45 p_url='https://linkedin.com/company/stone-river-elearning'\n",
    "id=57 p_url='https://linkedin.com/company/packt-publishing'\n",
    "count2=281 id=1755 p_url='https://linkedin.com/company/start-tech-academy'\n",
    "count2=282 id=1763 p_url='https://linkedin.com/company/youaccel/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = {'Index' : Index_company,\n",
    "        'LinkedIn Url' : Company_LinkedIn_url,\n",
    "    'Company Name' : Company_name,\n",
    "'Company work status' : Company_worksat,\n",
    "'Number of followers' : Company_followers,\n",
    "'Number of employees' : Number_of_employees,\n",
    "'About section' : About_Company,\n",
    "'Frequency of the latest post' : CPost1_frequency,\n",
    "'Number of reactions got for latest post' : CPost1_numberof_reactions,\n",
    "'Number of comments got for latest post' : CPost1_numberof_comments,\n",
    "'Frequency of the second latest post' : CPost2_frequency,\n",
    "'Number of reactions got for second latest post' : CPost2_numberof_reactions,\n",
    "'Number of comments got for second latest post' : CPost2_numberof_comments\n",
    " }\n",
    "\n",
    "li_comp_df = pd.DataFrame(data2)\n",
    "li_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc45861",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_comp_df.to_csv('LinkedIn_company_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c88d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
